{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "unique-entrepreneur",
   "metadata": {},
   "source": [
    "## We are going to attempt to answer the question of whether or not the hot hands effect exists in Tennis, using first and second serves. We want to answer these following questions:\n",
    "\n",
    "### 1. What is a player's chance of faulting their first serve, if their last first serve was a fault? (Equivalently, what is the chance of a player making their first serve, if their last serve went in).\n",
    "### 2. What is a player's chance of double faulting, if they double faulted the last point?\n",
    "### 3. What is a player's chance of an ace, if they had an ace the last point?\n",
    "\n",
    "### This amounts to calculating three conditional probabilities, i.e., for point 1 it is P(F|LF) where F is a fault on the current service and LF is a fault on the last service (LF for last-fault).\n",
    "### The \"hot hands\" effect exists if P(F|LG) > P(G|LG) , by a statistically significant amount (say 5 guassian sigma), where P(G|LG) is the probability of making the first serve, given that the last first serve went in. (LG for Last was Good).\n",
    "\n",
    "## An important point: We are going to use mock data to set up and verify the analysis, then we will dig up and find a data set that is optimal to use.\n",
    "### A couple of notes on why I am using mock data first: \n",
    "### First, it will allow me to weed out any bugs by working with a dataset where I know the answer. \n",
    "### Second, it will remove the possibility of biasing the analysis on real data. (I.e., I keep messing with my analysis until \"it works\" on the real data, this is closely tied to point 1). \n",
    "### Third, by creating my mock data set using values consistent with X years worth of data, I will have a really good estimate of how many years and players I need to aggregate to definitively answer the question -- potentially saving myself loads of time down the road. It turns out 3 years is about how much we need to extract a 10% hot hands effect with undeniable confidence (this is about 20,000 serves).\n",
    "### Fourth, is that although these datasets definitely exist -- which one to use is not immediately apparent. \n",
    "### At the pro level, the probability of an ace (and a double fault) is low (a typical number of aces in a given year is only 500, https://www.atptour.com/en/stats/aces/2019/all/all/ ). So point 2 and 3 might be exceedingly difficult to even investigate with confidence because we will be dealing with small number statistics. I.e., our constraints on the conditional probabilities will be weak. So making any kind of definitive conclusion will be hard. Looking at datasets at the college or highschool level would probably be more informative, (simply because the hot and cold hands effects, if they exist, are probably accentuated here) but then such datasets will be harder to come by.\n",
    "\n",
    "### So, without further ado, lets create this mock dataset, focusing on question 1 because:\n",
    "### Question 1 is likely to have the largest number of data points in the real dataset, and naively, the cold hands effect here should be just as present as it is in 2 and 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "remarkable-allen",
   "metadata": {},
   "source": [
    "## Section 1, generating a dataset that will be replaceable by real data at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "czech-sacrifice",
   "metadata": {},
   "source": [
    "Generally, top pro players make about 65 percent of their first serves and 90 percent of their second serves. Assigning 50 players whose aggregate first and second serve percentanges are random values between 50 and 70% would be pretty representative of the top 50 players. https://www.atptour.com/en/stats/1st-serve/2019/all/all/ -- but it also makes our mock data needlessly complicated. Instead, we will give the data a first serve percentage column, but we will set them all to 50%. This will make seeing the hot hands effect much easier.\n",
    "\n",
    "We are including the complicating factor here of multiple players because we will certainly need to aggregate over players in the real data in order to build up the sample size required.\n",
    "\n",
    "We want to be intelligent about how we structure this dataset. As with all questions, how you structure your data can be extremely hindering, or extremely enlightening. I personally prefer hefty datastructures that are easy to navigate. One could do this with a SQL table or something the like, but I will tackle with astropy tables because of their myriad of grouping and sorting abilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "funded-sucking",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.table import Table\n",
    "import numpy as np\n",
    "\n",
    "num_players = 50\n",
    "first_serve_chance_range = (0.5, 0.50001) # range between 50% and 50% :)\n",
    "player_table = {'player_id': np.arange(50), \n",
    "                'first_serve_chance': np.round(np.random.uniform(low=min(first_serve_chance_range), high=max(first_serve_chance_range), size=num_players), 3)}\n",
    "player_table = Table(player_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "severe-spokesman",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<i>Table length=5</i>\n",
       "<table id=\"table140462617848944\" class=\"table-striped table-bordered table-condensed\">\n",
       "<thead><tr><th>player_id</th><th>first_serve_chance</th></tr></thead>\n",
       "<thead><tr><th>int64</th><th>float64</th></tr></thead>\n",
       "<tr><td>0</td><td>0.5</td></tr>\n",
       "<tr><td>1</td><td>0.5</td></tr>\n",
       "<tr><td>2</td><td>0.5</td></tr>\n",
       "<tr><td>3</td><td>0.5</td></tr>\n",
       "<tr><td>4</td><td>0.5</td></tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Table length=5>\n",
       "player_id first_serve_chance\n",
       "  int64        float64      \n",
       "--------- ------------------\n",
       "        0                0.5\n",
       "        1                0.5\n",
       "        2                0.5\n",
       "        3                0.5\n",
       "        4                0.5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "player_table[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "planned-mixer",
   "metadata": {},
   "source": [
    "Great, now we have some random players with identification integers 0, 1..., 50 (we would replace their names with these integers in the real data set), and each has some aggregate first serve percentage. This would be fetched, in the real data, from something like the ATP website https://www.atptour.com/en/stats/aces/2019/all/all/ .\n",
    "\n",
    "Now, each player will play order-of-magnitude the same number of matches in a year (e.g., somewhere between 40 and 80). But we will need to take this into account when we assess the statistics, so we want to add this complication to the mock data as well. This will be the last complication we add."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "obvious-remainder",
   "metadata": {},
   "outputs": [],
   "source": [
    "serves_per_match = 150\n",
    "matches_played = (40, 80)\n",
    "years_of_data = 3\n",
    "num_serves_range = (serves_per_match * min(matches_played) * years_of_data, \n",
    "                    serves_per_match * max(matches_played) * years_of_data)\n",
    "\n",
    "player_table['num_serves_on_record'] = np.random.randint(low=min(num_serves_range), high=max(num_serves_range), size=len(player_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "organizational-webster",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<i>Table length=5</i>\n",
       "<table id=\"table140462617847744\" class=\"table-striped table-bordered table-condensed\">\n",
       "<thead><tr><th>player_id</th><th>first_serve_chance</th><th>num_serves_on_record</th></tr></thead>\n",
       "<thead><tr><th>int64</th><th>float64</th><th>int64</th></tr></thead>\n",
       "<tr><td>0</td><td>0.5</td><td>27093</td></tr>\n",
       "<tr><td>1</td><td>0.5</td><td>23411</td></tr>\n",
       "<tr><td>2</td><td>0.5</td><td>27732</td></tr>\n",
       "<tr><td>3</td><td>0.5</td><td>18141</td></tr>\n",
       "<tr><td>4</td><td>0.5</td><td>33256</td></tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Table length=5>\n",
       "player_id first_serve_chance num_serves_on_record\n",
       "  int64        float64              int64        \n",
       "--------- ------------------ --------------------\n",
       "        0                0.5                27093\n",
       "        1                0.5                23411\n",
       "        2                0.5                27732\n",
       "        3                0.5                18141\n",
       "        4                0.5                33256"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "player_table[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brutal-header",
   "metadata": {},
   "source": [
    "## Now we have all the aggregate data in hand to generate a mock data set. We will make a dataset class to generate these fake data, so that we can run the analysis for two different magnitudes of cold-hand effects:\n",
    "Recall that we are defining P(G|LG) as the probability of a good first serve, conditional on the last serve being good as well. And P(G|LF) as the probability of making the first serve, in the event that the previous first serve was a fault.\n",
    "\n",
    "## case 1: A hot hands effect of 10 % -- meaning P(G|LG) - P(G|LF)  = 0.1  (10% higher chance of making your serve if your last serve went in)\n",
    "## case 2: And no hot hand effect at all. I.e., an effect of 0% -- meaning P(G|LG) - P(G|LF) = 0.0\n",
    "\n",
    "### The goal is that, through our analysis later on, we can recover both instances of the hot-hands effect.\n",
    "### Remember that this is all statistical. If we succeed, we won't recover P(G|LG) - P(G|LF) = 0.1 for case 1, we will recover some posterior estimate that is consistent with (i.e., within 1 sigma of) 0.1\n",
    "\n",
    "### From another angle: all we are doing in case 1 is inducing a correlation between one serve and the next -- turning it into a correlated random walk. In case 2, this is a classic uncorrelated random walk.\n",
    "\n",
    "### We will make these mock data such that we have already split the data by service games. We wouldn't expect as large of a cold/hot hands effect that extends between one service game and the next (which could be separated by tens of minutes). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposite-beijing",
   "metadata": {},
   "source": [
    "We are going to simulate a set of serves using a monte carlo simulation. This is not the fastest way in terms of computation time to do this, but it is a reliable way that is very very simple to code up. Simple = less chance for bugs, so we are going to go with this method.\n",
    "\n",
    "After we code up the monte carlo method, we will write a class for the MockData to keep everything organized and accessible in a nice object-oriented way.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "lightweight-crown",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0, 0, 0, 0, 1, 0, 0]), array([0, 1, 1, 1, 0, 0, 1]), array([1, 1, 1, 1, 1, 1, 0]), array([0, 0, 0, 0, 1, 1, 0]), array([0, 1, 1, 0, 0, 0]), array([0, 0, 1, 0, 0, 0]), array([1, 1, 0, 1, 0, 0]), array([1, 1, 0, 1, 1, 0]), array([1, 1, 0, 0, 1, 0]), array([0, 0, 0, 0, 0, 0])]\n"
     ]
    }
   ],
   "source": [
    "def simulate_serves(num_serves, first_serve_chance, hot_hands_magnitude):\n",
    "    \"\"\"\n",
    "    Generate a long list of first serve statistics. 1 means the first serve went in, and 0 is a fault.\n",
    "    \"\"\"\n",
    "    cold_serve_chance = [1 - first_serve_chance, first_serve_chance]\n",
    "    hot_serve_chance = [1 - first_serve_chance - hot_hands_magnitude, first_serve_chance + hot_hands_magnitude]\n",
    "    serve_data = [np.random.choice([0, 1], p=hot_serve_chance)]\n",
    "    for i in range(1, num_serves):\n",
    "        if serve_data[i-1] == 1: # if the last serve went in:\n",
    "            serve_chance = hot_serve_chance\n",
    "        else: # the last serve did not go in:\n",
    "            serve_chance = cold_serve_chance\n",
    "        this_serve = np.random.choice([0, 1], p=serve_chance)\n",
    "        serve_data.append(this_serve)\n",
    "    return serve_data\n",
    "        \n",
    "    \n",
    "def split_serves_into_games(serve_data, num_serves_per_set):\n",
    "    \"\"\"\n",
    "    Tennis is played in discrete games. This function will split the long 1d list of service statistics into discrete service games.\n",
    "    \"\"\"\n",
    "    return np.array_split(serve_data, int(len(serve_data)/num_serves_per_set))\n",
    "\n",
    "# a quick test of the above functions\n",
    "serve_data = simulate_serves(10000, 0.5, 0.1)\n",
    "split_serve_data = split_serves_into_games(serve_data, 6)\n",
    "print(split_serve_data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "korean-optics",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "class MockData(object):\n",
    "    def __init__(self, player_table, serves_per_set=6, hot_hands_magnitude=0.1, serve_data=None):\n",
    "        self.player_table = player_table\n",
    "        self.serves_per_set = serves_per_set\n",
    "        self.hot_hands_magnitude = hot_hands_magnitude\n",
    "        if serve_data is None:\n",
    "            serve_data = self.init_serve_data()\n",
    "        self.serve_data = serve_data\n",
    "\n",
    "    def init_serve_data(self):\n",
    "        player_ids = self.player_table['player_id']\n",
    "        serve_data = {int(i): None for i in player_ids}\n",
    "        for i in serve_data.keys():\n",
    "            player_data = self.player_table[self.player_table['player_id'] == i][0]\n",
    "            serve_data[i] = simulate_serves(player_data['num_serves_on_record'],\n",
    "                                            player_data['first_serve_chance'],\n",
    "                                            self.hot_hands_magnitude)\n",
    "            serve_data[i] = split_serves_into_games(serve_data[i], self.serves_per_set)\n",
    "        return serve_data\n",
    "    \n",
    "    def write(self, path):\n",
    "        # save the serve_data dictionary to disc.\n",
    "        # convert the data to json serializable ints and lists. Json is human readable and wont restrict other people\n",
    "        # from using the data, compared to if we wrote out a pickled object.\n",
    "        for i in self.serve_data.keys():\n",
    "            self.serve_data[i] = [[int(kk) for kk in list(j)] for j in self.serve_data[i]]\n",
    "        with open(path, \"w\") as file:\n",
    "            json.dump(self.serve_data, file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "searching-vietnamese",
   "metadata": {},
   "source": [
    "Now here is the advantage of packaging everything into classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "genuine-description",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the simulation for a cold hands magnitude of 0.1\n",
    "case0p1 = MockData(player_table, hot_hands_magnitude=0.1)\n",
    "# run the simulation for a cold hands magnitude of 0.\n",
    "case0 = MockData(player_table, hot_hands_magnitude=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chinese-cannon",
   "metadata": {},
   "source": [
    "Now we just want to write the simulation data to disc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "unavailable-lithuania",
   "metadata": {},
   "outputs": [],
   "source": [
    "case0.write('formatted_mock_data/case0.json')\n",
    "case0p1.write('formatted_mock_data/case0p1.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unique-thing",
   "metadata": {},
   "source": [
    "## And I will see you in the actual analysis notebook: hot_hands_analysis_mockdata.ipynb, where we will load in these mock data and show that we can back out the truth from the mock data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immune-voice",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
